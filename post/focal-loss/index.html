<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 5.0.0-beta.0 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Jishnu Mukhoti">

  
  
  
    
  
  <meta name="description" content="Paper: Calibrating Deep Neural Networks using Focal Loss
What we want  Overparameterised classifier deep neural networks trained on the conventional cross-entropy objective are known to be overconfident and thus miscalibrated.">

  
  <link rel="alternate" hreflang="en-us" href="https://omegafragger.github.io/post/focal-loss/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css">

  




  

  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hub438bdc4799012c4d5051df5d1960bae_14185_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hub438bdc4799012c4d5051df5d1960bae_14185_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://omegafragger.github.io/post/focal-loss/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Jishnu Mukhoti">
  <meta property="og:url" content="https://omegafragger.github.io/post/focal-loss/">
  <meta property="og:title" content="How does focal loss help in calibrating deep neural networks? | Jishnu Mukhoti">
  <meta property="og:description" content="Paper: Calibrating Deep Neural Networks using Focal Loss
What we want  Overparameterised classifier deep neural networks trained on the conventional cross-entropy objective are known to be overconfident and thus miscalibrated."><meta property="og:image" content="https://omegafragger.github.io/post/focal-loss/featured.jpg">
  <meta property="twitter:image" content="https://omegafragger.github.io/post/focal-loss/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-11-18T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-11-19T10:25:09&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://omegafragger.github.io/post/focal-loss/"
  },
  "headline": "How does focal loss help in calibrating deep neural networks?",
  
  "image": [
    "https://omegafragger.github.io/post/focal-loss/featured.jpg"
  ],
  
  "datePublished": "2020-11-18T00:00:00Z",
  "dateModified": "2020-11-19T10:25:09Z",
  
  "author": {
    "@type": "Person",
    "name": "Jishnu Mukhoti"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Jishnu Mukhoti",
    "logo": {
      "@type": "ImageObject",
      "url": "https://omegafragger.github.io/images/icon_hub438bdc4799012c4d5051df5d1960bae_14185_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Paper: Calibrating Deep Neural Networks using Focal Loss\nWhat we want  Overparameterised classifier deep neural networks trained on the conventional cross-entropy objective are known to be overconfident and thus miscalibrated."
}
</script>

  

  


  


  





  <title>How does focal loss help in calibrating deep neural networks? | Jishnu Mukhoti</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  ">

  
  
  
    <script>window.wcDarkLightEnabled = true;</script>
  
  
    <script>const isSiteThemeDark = false;</script>
  
  
  <script src="/js/load-theme.js"></script>

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Jishnu Mukhoti</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Jishnu Mukhoti</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  




















  
  


<div class="article-container pt-3">
  <h1>How does focal loss help in calibrating deep neural networks?</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Nov 19, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  
  
  
  

  
  

</div>

  














</div>


<div class="article-header container-fluid featured-image-wrapper mt-4 mb-4" style="max-width: 1500px; max-height: 670px;">
  <div style="position: relative">
    <img src="/post/focal-loss/featured.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Paper: <a href="https://arxiv.org/abs/2002.09437" target="_blank" rel="noopener">Calibrating Deep Neural Networks using Focal Loss</a></p>
<h3 id="what-we-want">What we want</h3>
<ul>
<li>Overparameterised classifier deep neural networks trained on the conventional cross-entropy objective are known to be <a href="https://arxiv.org/abs/1706.04599" target="_blank" rel="noopener">overconfident and thus miscalibrated</a>.</li>
<li>With these networks being deployed in real-life applications like autonomous driving and medical diagnosis, it is imperative for them to predict with calibrated confidence estimates.</li>
<li>Ideally, we want a model which is <em>confident on its correct predictions</em>, produces <em>calibrated probability estimates</em> and also achieves <em>state-of-the-art test set accuracy</em>.</li>
</ul>
<h3 id="what-we-do">What we do</h3>
<ul>
<li>We explore an alternative loss function, <a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener"><strong>focal loss</strong></a> for training models.</li>
<li>Focal loss significantly <strong>outperforms cross-entropy</strong> and other baselines on calibration error scores (e.g. ECE) <strong>without compromising on test set accuracy</strong>.</li>
<li>This improvement in focal loss is present both <strong>pre and post temperature scaling</strong>.</li>
<li>We propose a sample-dependent adaptive way of deciding $\gamma$, the focal loss hyperparameter.</li>
<li>Finally, we show that training on focal loss can be better than post-hoc calibration using temperature scaling, especially on <strong>out-of-distribution samples</strong>.</li>
</ul>
<p align="center">
    <img src="reliability_plot_resnet110.png" width="650" alt="reliability diagrams" />
    <em>Fig. 1: Reliability diagrams for a ResNet110 trained on CIFAR-100</em>
</p>
<h2 id="what-causes-miscalibration">What Causes Miscalibration?</h2>
<ul>
<li>We train a ResNet-50 on CIFAR-10 using cross-entropy loss for 350 epochs.</li>
<li>The initial learning rate is 0.1 and it drops by a factor of 10 at epochs 150 and 250.</li>
<li>We plot the NLL and entropy of the softmax distribution over the course of training for both correctly and incorrectly classified train and test samples.</li>
</ul>
<p align="center">
    <img src="nll_overfitting.PNG" width="650" />
    <em>Fig. 2: NLL and softmax entropy computed over CIFAR-10 train and test sets (for correctly and incorrectly classified samples) over the course of training a ResNet-50 usign cross-entropy loss.</em>
</p>
<p>Two observations are worth noting from the above plots:</p>
<ol>
<li><strong>Curse of misclassified samples:</strong> The NLL overfitting is quite apparent after training epoch 150. Also, <em>the rise in test set NLL is only because of misclassified test samples</em>.</li>
<li><strong>Peak at the wrong place:</strong> While the test set NLL rises, the test set entropy decreases throughout training even for misclassified samples. Hence, <em>the model gets more and more confident on its predictions irrespective of their correctness</em>.</li>
</ol>
<p>We posit the reason behind the above observations as follows:</p>
<ul>
<li>Even after obtaining a 100% training accuracy, the optimiser can still reduce the loss further by increasing the model&rsquo;s confidences on its predictions.</li>
<li>In order to increase its confidence, the model <em>increases the magnitudes of the logits which in turn is achieved by increasing the magnitudes of its weights</em>.</li>
</ul>
<h2 id="improving-calibration-using-focal-loss">Improving Calibration using Focal Loss</h2>
<p>We explore an alternative loss function, <a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">focal loss</a>. Below are the forms of the two loss functions, cross-entropy (CE) and focal loss (Focal) (assuming one-hot ground truth encodings):</p>
<p>$$\mathcal{L}_{\mathrm{CE}} = - \log p$$</p>
<p>$$\mathcal{L}_{\mathrm{Focal}} = -(1-p)^\gamma \log p$$</p>
<p>In the above equations $p$ is the probability assigned by the model to the ground-truth correct class. When compared with cross-entropy, focal loss has an added $(1-p)^\gamma$ factor. The idea behind this is to give more preference to samples for which the model is placing less probability mass on the correct class. $\gamma$ is a hyperparameter.</p>
<h3 id="why-might-focal-loss-improve-calibration">Why might focal loss improve calibration?</h3>
<p><strong>Focal Loss minimises a regularised KL divergence.</strong>
We know that cross-entropy loss minimises the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="noopener">Kullback-Leibler (KL)</a> divergence between the target distribution $q$ over classes and the predicted softmax distribution $\hat{p}$. As it turns out, focal loss minimises a <em>regularised KL divergence</em> between the target and predicted distributions as shown below.</p>
<p>$$\mathcal{L}_{\mathrm{Focal}} \geq \mathrm{KL}(q||\hat{p}) - \gamma \mathbb{H}[\hat{p}]$$</p>
<ul>
<li>The regulariser is the entropy of the predicted distribution and the regularisation coefficient is $\gamma$.</li>
<li>Hence, focal loss tries to minimise the KL divergence between the predicted and target distributions while at the same time increasing the entropy of the predicted distribution.</li>
<li>Note that in the case of one-hot target distributions, only the component of entropy corresponding to the ground-truth correct class (i.e., $\gamma(-p \log p)$) is maximised.</li>
<li>This <em>prevents the predicted softmax distributions from becoming too peaky (low entropy)</em> and the model from becoming overconfident.</li>
</ul>
<h3 id="how-does-focal-loss-fare-on-nll-overfitting">How does focal loss fare on NLL overfitting?</h3>
<p>On the same training setup as above (i.e., ResNet-50 on CIFAR-10), if we use focal loss with $\gamma$ values set to 1, 2 and 3, the test set NLL vs training epochs plot is as follows.</p>
<p align="center">
    <img src="test_nll.png" width="400" />
    <em>Fig. 3: Test set NLL computed over different training epochs for ResNet-50 models trained using cross-entropy and focal loss with $\gamma$ values 1,2 and 3.</em>
</p>
<ul>
<li><strong>NLL overfitting is significantly reduced for models trained using focal loss!</strong></li>
<li>There also seems to be an ordering to this betterment where focal loss with $\gamma=3$ outperforms focal loss with $\gamma=2$, which outperforms the one with $\gamma=1$.</li>
</ul>
<h3 id="does-focal-loss-regularise-model-weights">Does focal loss regularise model weights?</h3>
<p>If we consider the gradient of focal loss with respect to the last layer model weights $w$ and the same for cross-entropy, we find that for a single sample, the following relation exists.</p>
<p>$$\frac{\partial\mathcal{L}_{\mathrm{Focal}}}{\partial w} = \frac{\partial\mathcal{L}}{\partial w}g(p, \gamma)$$</p>
<p>The factor $g(p, \gamma)$ is a function of both the predicted correct class probability $p$ and $\gamma$. On plotting $g(p, \gamma)$ against $p$ for different values of $\gamma$, we get the following plot.</p>
<p align="center">
    <img src="g_p_t_gamma.png" width="400" />
    <em>Fig. 4: $g(p, \gamma)$ vs $p$ plot.</em>
</p>
<ul>
<li>We see a pattern here, that higher $\gamma$ leads to higher gradient in focal loss for low values of $p$ and dying gradients for higher $p$.</li>
<li>We use this observation to <em>adaptively select</em> $\gamma$ <em>sample wise</em> such that the network can give more weight to samples with lower correct class probability. We call this training method as FLSD.</li>
<li>For each value of $\gamma$, $g(p, \gamma) &lt; 1$ when $p$ is above a certain threshold probability.</li>
<li>This means that during training, <em>once the network starts getting more confident on its predictions, the gradients for focal loss start having lower and lower norm</em> as compared to the gradients for cross-entropy.</li>
<li>Accordingly, the <em>weight norms for focal loss should also reduce as compared to that of cross-entropy</em>. To see this empirically, we plot the weight norms of the last linear layer for our training setup.</li>
</ul>
<p align="center">
    <img src="weight_norm.png" width="400" />
    <em>Fig. 5: Weight norm of the last linear layer over the course of training.</em>
</p>
<ul>
<li>For the first few training epochs, the focal loss weight norms are larger than cross-entropy.</li>
<li>However, after epoch 150, the point from where the network starts getting miscalibrated, <em>the ordering of weight norms completely reverses and focal loss has lower weight norms than cross-entropy</em>.</li>
</ul>
<p>This provides strong evidence in favour of our hypothesis that <strong>focal loss regularises weight norms in the network once the network achieves a certain level of confidence on its predictions</strong>.</p>
<h2 id="empirical-results">Empirical Results</h2>
<h3 id="classification-results-calibration-and-accuracy">Classification Results (Calibration and Accuracy)</h3>
<ul>
<li>We test the performance of focal loss on several datasets, architectures and using multiple different calibration error scores which include the <a href="https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf" target="_blank" rel="noopener">Expected Calibration Error</a> (ECE), Adaptive ECE (AdaECE), Classwise-ECE and others.</li>
<li>We also compare with loss baselines other than cross-entropy, like <a href="https://journals.ametsoc.org/mwr/article/78/1/1/96424/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF" target="_blank" rel="noopener">Brier Score</a>, <a href="http://proceedings.mlr.press/v80/kumar18a/kumar18a.pdf" target="_blank" rel="noopener">MMCE</a> and cross-entropy with <a href="https://arxiv.org/pdf/1906.02629.pdf" target="_blank" rel="noopener">label smoothing</a>.</li>
<li>We present error bars with confidence intervals for ECE, AdaECE and Classwise-ECE for ResNet-50 and ResNet-110 trained on <a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10</a> as well as the test set classification error for the same models below.</li>
</ul>
<p align="center">
    <img src="error_bars.PNG" />
    <em>Fig. 6: Error bar plots with confidence intervals for ECE, AdaECE and Classwise-ECE computed for ResNet-50 (first 3 from the left) and ResNet-110 (first 3 from the right) on CIFAR-10. T: post-temperature scaling</em>
</p>
<table>
<thead>
<tr>
<th>Model Architecture</th>
<th>Cross-Entropy</th>
<th>Brier Loss</th>
<th>MMCE</th>
<th>LS-0.05</th>
<th>FL-3 (Ours)</th>
<th>FLSD-53 (Ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ResNet-50</td>
<td>4.95</td>
<td>5.0</td>
<td>4.99</td>
<td>5.29</td>
<td>5.25</td>
<td>4.98</td>
</tr>
<tr>
<td>ResNet-110</td>
<td>4.89</td>
<td>5.48</td>
<td>5.4</td>
<td>5.52</td>
<td>5.08</td>
<td>5.42</td>
</tr>
</tbody>
</table>
<ul>
<li>Models trained using <strong>focal loss broadly outperform all other baselines on all the metrics both before and after temperature scaling</strong>!</li>
<li>In quite a few cases, the difference in performance is statistically significant.</li>
<li>Moreover, models trained on focal loss do not compromise on test set error or accuracy. All the models we train obtain test set classification errors in the same ballpark.</li>
</ul>
<h3 id="detection-of-ood-samples">Detection of OOD Samples</h3>
<p>We run the models trained using CIFAR-10 on test data drawn from the <a href="http://ufldl.stanford.edu/housenumbers" target="_blank" rel="noopener">SVHN</a> dataset which is out-of-distribution and consider the softmax entropy of these networks as a measure of uncertainty. For ResNet-110, the ROC plots obtained from this experiment are provided below.</p>
<p align="center">
    <img src="roc_plots.PNG" width="650"/>
    <em>Fig. 7: ROC plots obtained from ResNet-110 trained on CIFAR-10 and tested on SVHN. Left: pre-temperature scaling, right: post-temperature scaling.</em>
</p>
<ul>
<li>The results indicate that models trained on focal loss, with <em>higher AUROC</em>, are able to detect out-of-distribution samples from SVHN much better than models trained on cross-entropy loss.</li>
<li>What is particularly important to see here is that this improved performance is shown both pre and post temperature scaling!</li>
</ul>
<p>Thus, focal loss seems to be a good alternative to the conventionally used cross-entropy loss for producing confident and calibrated models without compromising on classification accuracy. <a href="https://arxiv.org/abs/2002.09437" target="_blank" rel="noopener">Our paper</a> provides with a lot more experiments and analysis, please have a look. <a href="https://github.com/torrvision/focal_calibration" target="_blank" rel="noopener">The code with all the pretrained models</a> is also available.</p>
<h2 id="citation-and-contact">Citation and Contact</h2>
<p>If the code or the paper has been useful in your research, please add a citation to our work:</p>
<pre><code>@article{mukhoti2020calibrating,
  title={Calibrating Deep Neural Networks using Focal Loss},
  author={Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip HS and Dokania, Puneet K},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
</code></pre>
<p>For any questions related to this work, please feel free to reach out to us at <a href="mailto:jishnu.mukhoti@eng.ox.ac.uk">jishnu.mukhoti@eng.ox.ac.uk</a> or <a href="mailto:viveka@robots.ox.ac.uk">viveka@robots.ox.ac.uk</a>.</p>

    </div>

    








<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://omegafragger.github.io/post/focal-loss/&amp;text=How%20does%20focal%20loss%20help%20in%20calibrating%20deep%20neural%20networks?" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://omegafragger.github.io/post/focal-loss/&amp;t=How%20does%20focal%20loss%20help%20in%20calibrating%20deep%20neural%20networks?" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=How%20does%20focal%20loss%20help%20in%20calibrating%20deep%20neural%20networks?&amp;body=https://omegafragger.github.io/post/focal-loss/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://omegafragger.github.io/post/focal-loss/&amp;title=How%20does%20focal%20loss%20help%20in%20calibrating%20deep%20neural%20networks?" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=How%20does%20focal%20loss%20help%20in%20calibrating%20deep%20neural%20networks?%20https://omegafragger.github.io/post/focal-loss/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://omegafragger.github.io/post/focal-loss/&amp;title=How%20does%20focal%20loss%20help%20in%20calibrating%20deep%20neural%20networks?" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://omegafragger.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/jishnu-mukhoti/avatar_huded731a639e9cbe36bb37cdb0b3ca709_22228_270x270_fill_q75_lanczos_center.jpg" alt="Jishnu Mukhoti"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://omegafragger.github.io/">Jishnu Mukhoti</a></h5>
      <h6 class="card-subtitle">DPhil (PhD) Student in Machine Learning</h6>
      <p class="card-text">My research interests include approximate Bayesian inference and uncertainty estimation in deep learning with applications in computer vision.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:jishnu@robots.ox.ac.uk" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/JishnuMukhoti" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=ebhcf9kAAAAJ&amp;hl" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/omegafragger" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/files/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    
    
    
    Published with
    <a href="https://wowchemy.com" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.0/mermaid.min.js" integrity="sha512-ja+hSBi4JDtjSqc4LTBsSwuBT3tdZ3oKYKd07lTVYmCnTCor56AnRql00ssqnTOR9Ss4gOP/ROGB3SfcJnZkeg==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/wowchemy.min.d9d80f811e95b4b4f6df1eaaf297b05f.js"></script>

    






</body>
</html>
