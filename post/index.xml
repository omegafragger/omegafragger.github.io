<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Jishnu Mukhoti</title>
    <link>https://omegafragger.github.io/post/</link>
      <atom:link href="https://omegafragger.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language>
    <image>
      <url>https://omegafragger.github.io/images/icon_hub438bdc4799012c4d5051df5d1960bae_14185_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://omegafragger.github.io/post/</link>
    </image>
    
    <item>
      <title>How does focal loss help in calibrating deep neural networks?</title>
      <link>https://omegafragger.github.io/post/focal-loss/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://omegafragger.github.io/post/focal-loss/</guid>
      <description>&lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2002.09437&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Calibrating Deep Neural Networks using Focal Loss&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;what-we-want&#34;&gt;What we want&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Overparameterised classifier deep neural networks trained on the conventional cross-entropy objective are known to be &lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;overconfident and thus miscalibrated&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;With these networks being deployed in real-life applications like autonomous driving and medical diagnosis, it is imperative for them to predict with calibrated confidence estimates.&lt;/li&gt;
&lt;li&gt;Ideally, we want a model which is &lt;em&gt;confident on its correct predictions&lt;/em&gt;, produces &lt;em&gt;calibrated probability estimates&lt;/em&gt; and also achieves &lt;em&gt;state-of-the-art test set accuracy&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-we-do&#34;&gt;What we do&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We explore an alternative loss function, &lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;focal loss&lt;/strong&gt;&lt;/a&gt; for training models.&lt;/li&gt;
&lt;li&gt;Focal loss significantly &lt;strong&gt;outperforms cross-entropy&lt;/strong&gt; and other baselines on calibration error scores (e.g. ECE) &lt;strong&gt;without compromising on test set accuracy&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;This improvement in focal loss is present both &lt;strong&gt;pre and post temperature scaling&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;We propose a sample-dependent adaptive way of deciding $\gamma$, the focal loss hyperparameter.&lt;/li&gt;
&lt;li&gt;Finally, we show that training on focal loss can be better than post-hoc calibration using temperature scaling, especially on &lt;strong&gt;out-of-distribution samples&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;reliability_plot_resnet110.png&#34; width=&#34;650&#34; alt=&#34;reliability diagrams&#34; /&gt;
    &lt;em&gt;Fig. 1: Reliability diagrams for a ResNet110 trained on CIFAR-100&lt;/em&gt;
&lt;/p&gt;
&lt;h2 id=&#34;what-causes-miscalibration&#34;&gt;What Causes Miscalibration?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We train a ResNet-50 on CIFAR-10 using cross-entropy loss for 350 epochs.&lt;/li&gt;
&lt;li&gt;The initial learning rate is 0.1 and it drops by a factor of 10 at epochs 150 and 250.&lt;/li&gt;
&lt;li&gt;We plot the NLL and entropy of the softmax distribution over the course of training for both correctly and incorrectly classified train and test samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;nll_overfitting.PNG&#34; width=&#34;650&#34; /&gt;
    &lt;em&gt;Fig. 2: NLL and softmax entropy computed over CIFAR-10 train and test sets (for correctly and incorrectly classified samples) over the course of training a ResNet-50 usign cross-entropy loss.&lt;/em&gt;
&lt;/p&gt;
&lt;p&gt;Two observations are worth noting from the above plots:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Curse of misclassified samples:&lt;/strong&gt; The NLL overfitting is quite apparent after training epoch 150. Also, &lt;em&gt;the rise in test set NLL is only because of misclassified test samples&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Peak at the wrong place:&lt;/strong&gt; While the test set NLL rises, the test set entropy decreases throughout training even for misclassified samples. Hence, &lt;em&gt;the model gets more and more confident on its predictions irrespective of their correctness&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We posit the reason behind the above observations as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Even after obtaining a 100% training accuracy, the optimiser can still reduce the loss further by increasing the model&amp;rsquo;s confidences on its predictions.&lt;/li&gt;
&lt;li&gt;In order to increase its confidence, the model &lt;em&gt;increases the magnitudes of the logits which in turn is achieved by increasing the magnitudes of its weights&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;improving-calibration-using-focal-loss&#34;&gt;Improving Calibration using Focal Loss&lt;/h2&gt;
&lt;p&gt;We explore an alternative loss function, &lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;focal loss&lt;/a&gt;. Below are the forms of the two loss functions, cross-entropy (CE) and focal loss (Focal) (assuming one-hot ground truth encodings):&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}_{\mathrm{CE}} = - \log p$$&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}_{\mathrm{Focal}} = -(1-p)^\gamma \log p$$&lt;/p&gt;
&lt;p&gt;In the above equations $p$ is the probability assigned by the model to the ground-truth correct class. When compared with cross-entropy, focal loss has an added $(1-p)^\gamma$ factor. The idea behind this is to give more preference to samples for which the model is placing less probability mass on the correct class. $\gamma$ is a hyperparameter.&lt;/p&gt;
&lt;h3 id=&#34;why-might-focal-loss-improve-calibration&#34;&gt;Why might focal loss improve calibration?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Focal Loss minimises a regularised KL divergence.&lt;/strong&gt;
We know that cross-entropy loss minimises the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kullback-Leibler (KL)&lt;/a&gt; divergence between the target distribution $q$ over classes and the predicted softmax distribution $\hat{p}$. As it turns out, focal loss minimises a &lt;em&gt;regularised KL divergence&lt;/em&gt; between the target and predicted distributions as shown below.&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}_{\mathrm{Focal}} \geq \mathrm{KL}(q||\hat{p}) - \gamma \mathbb{H}[\hat{p}]$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The regulariser is the entropy of the predicted distribution and the regularisation coefficient is $\gamma$.&lt;/li&gt;
&lt;li&gt;Hence, focal loss tries to minimise the KL divergence between the predicted and target distributions while at the same time increasing the entropy of the predicted distribution.&lt;/li&gt;
&lt;li&gt;Note that in the case of one-hot target distributions, only the component of entropy corresponding to the ground-truth correct class (i.e., $\gamma(-p \log p)$) is maximised.&lt;/li&gt;
&lt;li&gt;This &lt;em&gt;prevents the predicted softmax distributions from becoming too peaky (low entropy)&lt;/em&gt; and the model from becoming overconfident.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-does-focal-loss-fare-on-nll-overfitting&#34;&gt;How does focal loss fare on NLL overfitting?&lt;/h3&gt;
&lt;p&gt;On the same training setup as above (i.e., ResNet-50 on CIFAR-10), if we use focal loss with $\gamma$ values set to 1, 2 and 3, the test set NLL vs training epochs plot is as follows.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;test_nll.png&#34; width=&#34;400&#34; /&gt;
    &lt;em&gt;Fig. 3: Test set NLL computed over different training epochs for ResNet-50 models trained using cross-entropy and focal loss with $\gamma$ values 1,2 and 3.&lt;/em&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NLL overfitting is significantly reduced for models trained using focal loss!&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;There also seems to be an ordering to this betterment where focal loss with $\gamma=3$ outperforms focal loss with $\gamma=2$, which outperforms the one with $\gamma=1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;does-focal-loss-regularise-model-weights&#34;&gt;Does focal loss regularise model weights?&lt;/h3&gt;
&lt;p&gt;If we consider the gradient of focal loss with respect to the last layer model weights $w$ and the same for cross-entropy, we find that for a single sample, the following relation exists.&lt;/p&gt;
&lt;p&gt;$$\frac{\partial\mathcal{L}_{\mathrm{Focal}}}{\partial w} = \frac{\partial\mathcal{L}}{\partial w}g(p, \gamma)$$&lt;/p&gt;
&lt;p&gt;The factor $g(p, \gamma)$ is a function of both the predicted correct class probability $p$ and $\gamma$. On plotting $g(p, \gamma)$ against $p$ for different values of $\gamma$, we get the following plot.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;g_p_t_gamma.png&#34; width=&#34;400&#34; /&gt;
    &lt;em&gt;Fig. 4: $g(p, \gamma)$ vs $p$ plot.&lt;/em&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We see a pattern here, that higher $\gamma$ leads to higher gradient in focal loss for low values of $p$ and dying gradients for higher $p$.&lt;/li&gt;
&lt;li&gt;We use this observation to &lt;em&gt;adaptively select&lt;/em&gt; $\gamma$ &lt;em&gt;sample wise&lt;/em&gt; such that the network can give more weight to samples with lower correct class probability. We call this training method as FLSD.&lt;/li&gt;
&lt;li&gt;For each value of $\gamma$, $g(p, \gamma) &amp;lt; 1$ when $p$ is above a certain threshold probability.&lt;/li&gt;
&lt;li&gt;This means that during training, &lt;em&gt;once the network starts getting more confident on its predictions, the gradients for focal loss start having lower and lower norm&lt;/em&gt; as compared to the gradients for cross-entropy.&lt;/li&gt;
&lt;li&gt;Accordingly, the &lt;em&gt;weight norms for focal loss should also reduce as compared to that of cross-entropy&lt;/em&gt;. To see this empirically, we plot the weight norms of the last linear layer for our training setup.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;weight_norm.png&#34; width=&#34;400&#34; /&gt;
    &lt;em&gt;Fig. 5: Weight norm of the last linear layer over the course of training.&lt;/em&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the first few training epochs, the focal loss weight norms are larger than cross-entropy.&lt;/li&gt;
&lt;li&gt;However, after epoch 150, the point from where the network starts getting miscalibrated, &lt;em&gt;the ordering of weight norms completely reverses and focal loss has lower weight norms than cross-entropy&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This provides strong evidence in favour of our hypothesis that &lt;strong&gt;focal loss regularises weight norms in the network once the network achieves a certain level of confidence on its predictions&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;empirical-results&#34;&gt;Empirical Results&lt;/h2&gt;
&lt;h3 id=&#34;classification-results-calibration-and-accuracy&#34;&gt;Classification Results (Calibration and Accuracy)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We test the performance of focal loss on several datasets, architectures and using multiple different calibration error scores which include the &lt;a href=&#34;https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Expected Calibration Error&lt;/a&gt; (ECE), Adaptive ECE (AdaECE), Classwise-ECE and others.&lt;/li&gt;
&lt;li&gt;We also compare with loss baselines other than cross-entropy, like &lt;a href=&#34;https://journals.ametsoc.org/mwr/article/78/1/1/96424/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brier Score&lt;/a&gt;, &lt;a href=&#34;http://proceedings.mlr.press/v80/kumar18a/kumar18a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MMCE&lt;/a&gt; and cross-entropy with &lt;a href=&#34;https://arxiv.org/pdf/1906.02629.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;label smoothing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;We present error bars with confidence intervals for ECE, AdaECE and Classwise-ECE for ResNet-50 and ResNet-110 trained on &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CIFAR-10&lt;/a&gt; as well as the test set classification error for the same models below.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;error_bars.PNG&#34; /&gt;
    &lt;em&gt;Fig. 6: Error bar plots with confidence intervals for ECE, AdaECE and Classwise-ECE computed for ResNet-50 (first 3 from the left) and ResNet-110 (first 3 from the right) on CIFAR-10. T: post-temperature scaling&lt;/em&gt;
&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Architecture&lt;/th&gt;
&lt;th&gt;Cross-Entropy&lt;/th&gt;
&lt;th&gt;Brier Loss&lt;/th&gt;
&lt;th&gt;MMCE&lt;/th&gt;
&lt;th&gt;LS-0.05&lt;/th&gt;
&lt;th&gt;FL-3 (Ours)&lt;/th&gt;
&lt;th&gt;FLSD-53 (Ours)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ResNet-50&lt;/td&gt;
&lt;td&gt;4.95&lt;/td&gt;
&lt;td&gt;5.0&lt;/td&gt;
&lt;td&gt;4.99&lt;/td&gt;
&lt;td&gt;5.29&lt;/td&gt;
&lt;td&gt;5.25&lt;/td&gt;
&lt;td&gt;4.98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet-110&lt;/td&gt;
&lt;td&gt;4.89&lt;/td&gt;
&lt;td&gt;5.48&lt;/td&gt;
&lt;td&gt;5.4&lt;/td&gt;
&lt;td&gt;5.52&lt;/td&gt;
&lt;td&gt;5.08&lt;/td&gt;
&lt;td&gt;5.42&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Models trained using &lt;strong&gt;focal loss broadly outperform all other baselines on all the metrics both before and after temperature scaling&lt;/strong&gt;!&lt;/li&gt;
&lt;li&gt;In quite a few cases, the difference in performance is statistically significant.&lt;/li&gt;
&lt;li&gt;Moreover, models trained on focal loss do not compromise on test set error or accuracy. All the models we train obtain test set classification errors in the same ballpark.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;detection-of-ood-samples&#34;&gt;Detection of OOD Samples&lt;/h3&gt;
&lt;p&gt;We run the models trained using CIFAR-10 on test data drawn from the &lt;a href=&#34;http://ufldl.stanford.edu/housenumbers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SVHN&lt;/a&gt; dataset which is out-of-distribution and consider the softmax entropy of these networks as a measure of uncertainty. For ResNet-110, the ROC plots obtained from this experiment are provided below.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
    &lt;img src=&#34;roc_plots.PNG&#34; width=&#34;650&#34;/&gt;
    &lt;em&gt;Fig. 7: ROC plots obtained from ResNet-110 trained on CIFAR-10 and tested on SVHN. Left: pre-temperature scaling, right: post-temperature scaling.&lt;/em&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The results indicate that models trained on focal loss, with &lt;em&gt;higher AUROC&lt;/em&gt;, are able to detect out-of-distribution samples from SVHN much better than models trained on cross-entropy loss.&lt;/li&gt;
&lt;li&gt;What is particularly important to see here is that this improved performance is shown both pre and post temperature scaling!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, focal loss seems to be a good alternative to the conventionally used cross-entropy loss for producing confident and calibrated models without compromising on classification accuracy. &lt;a href=&#34;https://arxiv.org/abs/2002.09437&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Our paper&lt;/a&gt; provides with a lot more experiments and analysis, please have a look. &lt;a href=&#34;https://github.com/torrvision/focal_calibration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The code with all the pretrained models&lt;/a&gt; is also available.&lt;/p&gt;
&lt;h2 id=&#34;citation-and-contact&#34;&gt;Citation and Contact&lt;/h2&gt;
&lt;p&gt;If the code or the paper has been useful in your research, please add a citation to our work:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{mukhoti2020calibrating,
  title={Calibrating Deep Neural Networks using Focal Loss},
  author={Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip HS and Dokania, Puneet K},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For any questions related to this work, please feel free to reach out to us at &lt;a href=&#34;mailto:jishnu.mukhoti@eng.ox.ac.uk&#34;&gt;jishnu.mukhoti@eng.ox.ac.uk&lt;/a&gt; or &lt;a href=&#34;mailto:viveka@robots.ox.ac.uk&#34;&gt;viveka@robots.ox.ac.uk&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
